{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==1.5.3 numpy==1.26.4 openai==1.37.1 langchain_community==0.2.9 langchain_text_splitters==0.2.2 langchain_openai==0.1.17 statsmodels==0.14.3 scipy==1.14.0 replicate==0.32.1 Scikit-learn==1.5.1 python-dotenv==1.0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RIS dataset\n",
    "df = pd.read_csv(filepath_or_buffer='/path/document.csv', sep=';')\n",
    "\n",
    "#Delete unneccessary columns & reorder dataframe\n",
    "df = df.drop(columns=[])\n",
    "new_order =['new order']\n",
    "df_neworder = df.reindex(columns=new_order)\n",
    "\n",
    "#Rename MRI devices with their respectiv magnetic flux density\n",
    "df_devices = df_neworder.replace({'MRI device':'magnetic flux density'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract paragraphs based on the starting keyword\n",
    "def extract_paragraph(text, start_keywords):\n",
    "    # Create a regex pattern that matches any of the start keywords\n",
    "    start_pattern = '|'.join([re.escape(keyword) for keyword in start_keywords])\n",
    "    pattern = re.compile(rf'({start_pattern})(.*?)(?=(\\n[A-Z][^:]*:|\\Z))', re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "# Keywords for clinical question & imaging procedure description\n",
    "klinik_keywords = ['keywords']\n",
    "technik_keywords = ['keywords']\n",
    "\n",
    "# Apply the function to extract the required paragraphs and store in a new column\n",
    "df_devices['new column clinical question'] = df_devices['report'].apply(lambda x: extract_paragraph(x, klinik_keywords))\n",
    "df_devices['new column sequences'] = df_devices['report'].apply(lambda x: extract_paragraph(x, technik_keywords))\n",
    "\n",
    "# Delete the originial report column\n",
    "df_extracted = df_devices.drop(columns=['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for contrast medium administration\n",
    "keywords = ['keywords']\n",
    "\n",
    "# Function to check if any keyword is in the text\n",
    "def contains_keywords(text, keywords):\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "# Create a new list to store the results\n",
    "contrast_medium = df_extracted['sequences'].apply(lambda x: contains_keywords(x, keywords))\n",
    "\n",
    "# Add the list as a new column to the DataFrame\n",
    "df_extracted['new column contrast medium administration'] = contrast_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save result in a new CSV file\n",
    "df_extracted.to_csv('/path/standardized document.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROTOCOL PREDICTION OPEN SOURCE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import replicate\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA 3.1 405B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data with clinical question\n",
    "df = pd.read_csv(filepath_or_buffer='/path/clinical_questions.csv', sep=';')\n",
    "\n",
    "#Load enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "#Set API Token\n",
    "replicate_api_token = os.getenv('REPLICATE_API_KEY')\n",
    "replicate_client = replicate.Client(api_token=replicate_api_token)\n",
    "\n",
    "standard_sequences = ['Axiale T1','Coronare T1','Axiale T2','Sagittale T2','Axiale T2 FS','Coronare T2 FS','Axiale T2*','Axiale STIR','Sagittale STIR','Coronare TIR','Axiale T2 BLADE','Axiale T1 FLASH','Axiale T2 FLASH','Sagittale T1 FLASH','Coronare T1 FLASH','Axiale T2 SPACE','Sagittale T2 SPACE','Coronare FLAIR','Axiale FLAIR','Sagittale FLAIR','Coronare FLAIR FAST','3D FLAIR','3D T1 SPACE FS','3D T2 SPACE FS','T1 MPRAGE','T1 VIBE','Coronare T1 Dynamik','Axiale SWI','Axiale DWI','Coronare DWI','Axiale DTI','Art. TOF-MRA']\n",
    "\n",
    "#Prediction Pipeline\n",
    "def prediction_prompt(row):\n",
    "    input = {\n",
    "    \"top_p\": 0.1,\n",
    "    \"prompt\": (\n",
    "        f\"Du bist Neuroradiologe. Du bekommst eine radiologische Fragestellung.\\n\"\n",
    "        f\"Die Fragestellung enthält Abkürzungen. Formuliere dafür die Abkürzungen aus und antworte so: Abkürzung:ausgeschriebene Abkürzung.\\n\"\n",
    "        f\"Wenn es bereits eine Diagnose gibt, nenne diese. Führe dann die 3 wahrscheinlichsten Differentialdiagnosen für diese Fragestellung auf.\\n\"\n",
    "        f\"Dir stehen standardisierte MRT Sequenzen zur Verfügung: {standard_sequences}.\\n\"\n",
    "        f\"Nenne alle Sequenzen aus der Liste der standardisierten MRT Sequenzen, die in der MRT Untersuchung nötig sind, um die Fragestellung zu beantworten. Falls eine Sequenz vor und nach der Kontrastmittelgabe geplant werden soll, nenne sie zweifach.\\n\"\n",
    "        f\"Bestimme außerdem, ob in der MRT Untersuchung Kontrastmittel gegeben werden soll oder nicht. Wenn ja, schreibe TRUE, wenn nicht, schreibe FALSE.\\n\"\n",
    "        f\"Befolge stets das Antwortformat. Erkläre dein Vorgehen nicht. Gib nur die erfragten Informationen wieder, füge keine weiteren Zeichen hinzu.\\n\"\n",
    "        f\"Deine Antwort soll folgendes Format haben:\\n\"\n",
    "        f\"Abkürzungen: Abkürzung:ausgeschriebene Abkürzung, Abkürzung:ausgeschriebene Abkürzung, ...\\n\"\n",
    "        f\"Diagnose: bereits bekannte Diagnose, Differentialdiagnosen: 1. wahrscheinlichste, 2. zweitwahrscheinlichste, 3. drittwahrscheinlichste\\n\"\n",
    "        f\"Sequenzen: Sequenz 1, Sequenz 2, Sequenz 3, ...\\n\"\n",
    "        f\"Kontrastmittelgabe: TRUE oder FALSE\\n\"\n",
    "        f\"Radiologische Fragestellung: {row['Fragestellung']}\"),\n",
    "    \"min_tokens\": 0,\n",
    "    \"temperature\": 0.1,\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"presence_penalty\": 1.15\n",
    "}\n",
    "    # Using Replicate's API to run the prediction\n",
    "    output = replicate_client.run('meta/meta-llama-3.1-405b-instruct', input=input)\n",
    "    \n",
    "    # Join tokens into a coherent string\n",
    "    if isinstance(output, list):\n",
    "        output = ''.join(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Iterate over the test DataFrame and generate predictions\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():  \n",
    "    result = prediction_prompt(row)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE DATA OF THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the extracted data\n",
    "abbreviations = []\n",
    "differential_diagnosis = []\n",
    "sequence_prediction = []\n",
    "contrastmedium_prediction = []\n",
    "\n",
    "# Iterate through each entry in the results\n",
    "for entry in results:\n",
    "    # Match the 'Abkürzungen:' section\n",
    "    abbreviation_match = re.search(r'Abkürzungen:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if abbreviation_match:\n",
    "        abbreviations.append(abbreviation_match.group(1).strip())\n",
    "    else:\n",
    "        abbreviations.append(None)  # Append None if the section is missing\n",
    "\n",
    "    # Match the 'Differentialdiagnosen:' section\n",
    "    dd_match = re.search(r'Diagnose:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if dd_match:\n",
    "        differential_diagnosis.append(dd_match.group(1).strip())\n",
    "    else:\n",
    "        differential_diagnosis.append(None)  # Append an None if the section is missing\n",
    "    \n",
    "    # Match the 'Sequenzen:' section\n",
    "    sequenzen_match = re.search(r'Sequenzen:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if (sequenzen_match):\n",
    "        sequence_prediction.append(sequenzen_match.group(1).strip())\n",
    "    else:\n",
    "        sequence_prediction.append(None)  # Append None if the section is missing\n",
    "\n",
    "    # Match the 'Kontrastmittelgabe:' section\n",
    "    contrastmedium_match = re.search(r'Kontrastmittelgabe:\\s*(TRUE|FALSE)', entry, re.DOTALL)\n",
    "    if contrastmedium_match:\n",
    "        contrastmedium_prediction.append(contrastmedium_match.group(1).strip())\n",
    "    else:\n",
    "        contrastmedium_prediction.append(None)  # Append None if the section is missing\n",
    "\n",
    "\n",
    "#Add the extracted results to the dataframe\n",
    "df.insert(8,'Abkürzungen',abbreviations)\n",
    "df.insert(9,'Diagnose/DD',differential_diagnosis)\n",
    "df.insert(10,'Vorhersage Sequenzen', sequence_prediction)\n",
    "df.insert(11,'Vorhersage Kontrastmittelgabe', contrastmedium_prediction)\n",
    "\n",
    "#Save the dataframe as csv\n",
    "df.to_csv(path_or_buf='/path/results',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA 3.1 405B WITH RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with clinical question\n",
    "df = pd.read_csv(filepath_or_buffer='/path/clinical questions.csv', sep=';')\n",
    "\n",
    "#Load enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Token\n",
    "replicate_api_token = os.getenv('REPLICATE_API_KEY')\n",
    "replicate_client = replicate.Client(api_token=replicate_api_token)\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to load and split PDF documents\n",
    "def get_docs():\n",
    "    loader_pdf = PyPDFLoader('/path/guidelines')\n",
    "    pdf_doc = loader_pdf.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n \\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    splitpdf = text_splitter.split_documents(pdf_doc)\n",
    "    return splitpdf\n",
    "\n",
    "# Function to create the vector store\n",
    "def create_vector_store(docs):\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-ada-002\")\n",
    "    vectorStore = FAISS.from_documents(docs, embedding=embedding)\n",
    "    return vectorStore\n",
    "\n",
    "docs = get_docs()\n",
    "vectorStore = create_vector_store(docs)\n",
    "\n",
    "\n",
    "#Prediction Pipeline\n",
    "def prediction_prompt(row):\n",
    "    #Embed clinical question into query\n",
    "    query = f\"Das ist eine radiologische Fragestellung: {row['Fragestellung']}. Zu welcher 'Anwendung' gehört sie? Das benutzte 'Gerät' ist {row['Gerät']}.\"\n",
    "    \n",
    "    #Evoke retrieval of documents\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    input = {\n",
    "    \"top_p\": 0.1,\n",
    "    \"prompt\": (\n",
    "        f\"Du bist Neuroradiologe. Du bekommst eine radiologische Fragestellung.\\n\"\n",
    "        f\"Die Fragestellung enthält Abkürzungen. Formuliere nur die Abkürzungen aus der Fragestellung aus und antworte so: Abkürzung:ausgeschriebene Abkürzung.\\n\"\n",
    "        f\"Wenn es bereits eine Diagnose gibt, nenne diese. Führe dann die 3 wahrscheinlichsten Differentialdiagnosen für diese Fragestellung auf.\\n\"\n",
    "        f\"Du hast 4 MRT-Protokolle zur Verfügung: {retrieved_docs}. Jedes Protokoll enthält einen Abschnitt zur Anwendung. Dieser behinhaltet für welche radiologischen Fragestellungen das jeweilige Protokoll geeignet ist.\\n\"\n",
    "        f\"Wähle das Protokoll aus, was am besten zur Beantwortung dieser radiologischen Fragestellung passt. Berücksichtige hierbei vorallem die Details aus Anwendungsbeschreibung des Protokolls.\\n\"\n",
    "        f\"Gib die Informationen des Protokolls unverändert wieder. Befolge stets das Antwortformat. Erkläre dein Vorgehen nicht, füge keine weiteren Zeichen hinzu.\\n\"\n",
    "        f\"Deine Antwort soll folgendes Format haben:\\n\"\n",
    "        f\"Abkürzungen: Abkürzung:ausgeschriebene Abkürzung, Abkürzung:ausgeschriebene Abkürzung, ...\\n\"\n",
    "        f\"Diagnose: bereits bekannte Diagnose, Differentialdiagnosen: 1. wahrscheinlichste, 2. zweitwahrscheinlichste, 3. drittwahrscheinlichste\\n\"\n",
    "        f\"Protokollname: Name des Protokolls\\n\"\n",
    "        f\"Sequenzen: Sequenzen des Protokolls\\n\"\n",
    "        f\"Kontrastmittelgabe: Kontrastmittelgabe im Protokoll\\n\"\n",
    "        f\"Radiologische Fragestellung: {row['Fragestellung']}\"),\n",
    "    \"min_tokens\": 0,\n",
    "    \"temperature\": 0.1,\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"presence_penalty\": 1.15\n",
    "}\n",
    "    # Using Replicate's API to run the prediction\n",
    "    output = replicate_client.run('meta/meta-llama-3.1-405b-instruct', input=input)\n",
    "    \n",
    "    # Join tokens into a coherent string\n",
    "    if isinstance(output, list):\n",
    "        output = ''.join(output)\n",
    "\n",
    "    return output, retrieved_docs\n",
    "\n",
    "# Iterate over the test DataFrame and generate predictions\n",
    "results = []\n",
    "retrieved_documents = []\n",
    "\n",
    "for index, row in df.iterrows():  \n",
    "    result, retrieved_docs = prediction_prompt(row)\n",
    "    results.append(result)\n",
    "    retrieved_documents.append(retrieved_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT THE DATA OF THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs_name = []\n",
    "\n",
    "# Iterate through each list in the retrieved_documents and extract protocol name\n",
    "for entry_list in retrieved_documents:\n",
    "    name_group = []\n",
    "\n",
    "    if isinstance(entry_list, list):\n",
    "        # Iterate through each document-like object in the list\n",
    "        for entry in entry_list:\n",
    "            # Check if the entry has 'page_content'\n",
    "            if hasattr(entry, 'page_content'):\n",
    "                page_content = entry.page_content\n",
    "                \n",
    "                # Split the content by lines\n",
    "                lines = page_content.split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    # Adjust to match both 'Name :' and 'Name:'\n",
    "                    if re.match(r\"Name\\s*:\", line.strip()):\n",
    "                        # Extract the part after 'Name:'\n",
    "                        name = line.split(\":\", 1)[1].strip()\n",
    "                        name_group.append(name)\n",
    "\n",
    "    # Append the group of names (per row) to the main list\n",
    "    retrieved_docs_name.append(name_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the extracted data\n",
    "abbreviations = []\n",
    "differential_diagnosis = []\n",
    "protocol_prediction = []\n",
    "sequence_prediction = []\n",
    "contrastmedium_prediction = []\n",
    "\n",
    "# Iterate through each entry in the results\n",
    "for entry in results:\n",
    "    # Match the 'Abkürzungen:' section\n",
    "    abbreviation_match = re.search(r'Abkürzungen:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if abbreviation_match:\n",
    "        abbreviations.append(abbreviation_match.group(1).strip())\n",
    "    else:\n",
    "        abbreviations.append(None)  # Append None if the section is missing\n",
    "\n",
    "    # Match the 'Differentialdiagnosen:' section\n",
    "    dd_match = re.search(r'Diagnose:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if dd_match:\n",
    "        differential_diagnosis.append(dd_match.group(1).strip())\n",
    "    else:\n",
    "        differential_diagnosis.append(None)  # Append an None if the section is missing\n",
    "    \n",
    "    # Match the 'Protokollname:' section\n",
    "    protocol_match = re.search(r'Protokollname:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if protocol_match:\n",
    "        protocol_prediction.append(protocol_match.group(1).strip())\n",
    "    else:\n",
    "        protocol_prediction.append(None)  # Append None if the section is missing\n",
    "\n",
    "    # Match the 'Sequenzen:' section\n",
    "    sequenzen_match = re.search(r'Sequenzen:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if sequenzen_match:\n",
    "        sequence_prediction.append(sequenzen_match.group(1).strip())\n",
    "    else:\n",
    "        sequence_prediction.append(None)  # Append None if the section is missing\n",
    "\n",
    "    # Match the 'Kontrastmittelgabe:' section\n",
    "    contrastmedium_match = re.search(r'Kontrastmittelgabe:\\s*(ja|nein|gegebenenfalls)', entry, re.DOTALL)\n",
    "    if contrastmedium_match:\n",
    "        contrastmedium_prediction.append(contrastmedium_match.group(1).strip())\n",
    "    else:\n",
    "        contrastmedium_prediction.append(None)  # Append None if the section is missing\n",
    "\n",
    "#Add extrated results to dataframe \n",
    "df.insert(7,'Abkürzungen',abbreviations)\n",
    "df.insert(8,'Diagnose/DD',differential_diagnosis)\n",
    "df.insert(9,'Vorhersage Protokollname',protocol_prediction)\n",
    "df.insert(10,'Vorhersage Sequenzen', sequence_prediction)\n",
    "df.insert(11,'Vorhersage Kontrastmittelgabe', contrastmedium_prediction)\n",
    "df.insert(12,'Retrieved Documents',retrieved_docs_name)\n",
    "\n",
    "#Save the dataframe as CSV\n",
    "df.to_csv(path_or_buf='/path/results',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROTOCOL PREDICTION PROPRIETARY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import re\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Load the data with clinical questions\n",
    "df = pd.read_csv(filepath_or_buffer='/path/clinical question.csv', sep=',')\n",
    "\n",
    "standard_sequences = ['Axiale T1','Coronare T1','Axiale T2','Sagittale T2','Axiale T2 FS','Coronare T2 FS','Axiale T2*','Axiale STIR','Sagittale STIR','Coronare TIR','Axiale T2 BLADE','Axiale T1 FLASH','Axiale T2 FLASH','Sagittale T1 FLASH','Coronare T1 FLASH','Axiale T2 SPACE','Sagittale T2 SPACE','Coronare FLAIR','Axiale FLAIR','Sagittale FLAIR','Coronare FLAIR FAST','3D FLAIR','3D T1 SPACE FS','3D T2 SPACE FS','T1 MPRAGE','T1 VIBE','Coronare T1 Dynamik','Axiale SWI','Axiale DWI','Coronare DWI','Axiale DTI','Art. TOF-MRA']\n",
    "\n",
    "def prediction_prompt(standard_sequences,row):\n",
    "    prompt = (\n",
    "        f\"Du bist Neuroradiologe. Du bekommst eine radiologische Fragestellung.Radiologische Fragestellung: {row['Fragestellung']}\\n\"\n",
    "        f\"Dir stehen standardisierte MRT Sequenzen zur Verfügung: {standard_sequences}.\\n\"\n",
    "        f\"Nenne alle Sequenzen aus der Liste der standardisierten MRT Sequenzen, die in der MRT Untersuchung nötig sind, um die Fragestellung zu beantworten. Falls eine Sequenz vor und nach der Kontrastmittelgabe geplant werden soll, nenne sie zweifach.\\n\"\n",
    "        f\"Bestimme außerdem, ob in der MRT Untersuchung Kontrastmittel gegeben werden soll oder nicht. Wenn ja, schreibe TRUE, wenn nicht, schreibe FALSE.\\n\"\n",
    "        f\"Befolge stets das Antwortformat. Erkläre dein Vorgehen nicht. Gib nur die erfragten Informationen wieder, füge keine weiteren Zeichen hinzu.\\n\"\n",
    "        f\"Deine Antwort soll folgendes Format haben:\\n\"\n",
    "        f\"Sequenzen: Sequenz 1, Sequenz 2, Sequenz 3, ...\\n\"\n",
    "        f\"Kontrastmittelgabe: TRUE oder FALSE\\n\"\n",
    "    )\n",
    "    \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a neuroradiologist.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    result = completion.choices[0].message.content.strip()\n",
    "\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "# Iterate over the test DataFrame and generate predictions\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    result = prediction_prompt(standard_sequences,row)\n",
    "    results.append(result)\n",
    "\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT DATA OF THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the extracted data\n",
    "abbreviations = []\n",
    "differential_diagnosis = []\n",
    "sequence_prediction = []\n",
    "contrastmedium_prediction= []\n",
    "\n",
    "# Iterate through each entry in the results\n",
    "for entry in results:\n",
    "    # Match the 'Abkürzungen:' section\n",
    "    abbreviation_match = re.search(r'Abkürzungen:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if abbreviation_match:\n",
    "        abbreviations.append(abbreviation_match.group(1).strip())\n",
    "\n",
    "    # Match the 'Differentialdiagnosen:' section\n",
    "    dd_match = re.search(r'Diagnose:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if dd_match:\n",
    "        # Extract the list of differential diagnoses\n",
    "        dd_list = [dd.strip() for dd in dd_match.group(1).split(',')]\n",
    "        differential_diagnosis.append(dd_list)\n",
    "\n",
    "    # Match the 'Sequenzen:' section\n",
    "    sequenzen_match = re.search(r'Sequenzen:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if sequenzen_match:\n",
    "        sequence_prediction.append(sequenzen_match.group(1).strip())\n",
    "\n",
    "    # Match the 'Kontrastmittelgabe:' section\n",
    "    contrastmedium_match = re.search(r'Kontrastmittelgabe:\\s*(TRUE|FALSE)', entry, re.DOTALL)\n",
    "    if contrastmedium_match:\n",
    "        contrastmedium_prediction.append(contrastmedium_match.group(1).strip())\n",
    "\n",
    "#Add extracted results to the dataframe\n",
    "df.insert(8,'Abkürzungen', abbreviations)\n",
    "df.insert(9,'Differentialdiagnosen', differential_diagnosis)\n",
    "df.insert(10,'Vorhersage Sequenzen', sequence_prediction)\n",
    "df.insert(11,'Vorhersage Kontrastmittelgabe', contrastmedium_prediction)\n",
    "\n",
    "#Save as csv\n",
    "df.to_csv(path_or_buf='/path/results.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(filepath_or_buffer='path/clinical questions.csv', sep=';')\n",
    "\n",
    "# Function to load and split the PDF document\n",
    "def get_docs():\n",
    "    loader_pdf = PyPDFLoader('/path/guidelines.pdf')\n",
    "    pdf_doc = loader_pdf.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n \\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=450,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    splitpdf = text_splitter.split_documents(pdf_doc)\n",
    "    return splitpdf\n",
    "\n",
    "# Function to create the vector store\n",
    "def create_vector_store(docs):\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-ada-002\")\n",
    "    vectorStore = FAISS.from_documents(docs, embedding=embedding)\n",
    "    return vectorStore\n",
    "\n",
    "docs = get_docs()\n",
    "vectorStore = create_vector_store(docs)\n",
    "\n",
    "# Prediction Pipeline\n",
    "def prediction_prompt(row):\n",
    "    #Embed clinical question into query\n",
    "    query = f\"Das ist eine radiologische Fragestellung:{row['Fragestellung']}. Zu welcher 'Anwendung' gehört sie? Das benutzte 'Gerät' ist {row['Gerät']}.\"\n",
    "\n",
    "    #Evoke retrieval of documents\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Du bist Neuroradiologe. Du bekommst eine radiologische Fragestellung.\\n\"\n",
    "        f\"Die Fragestellung enthält Abkürzungen. Formuliere nur die Abkürzungen aus der Fragestellung aus und antworte so: Abkürzung:ausgeschriebene Abkürzung.\\n\"\n",
    "        f\"Wenn es bereits eine Diagnose gibt, nenne diese. Führe dann die 3 wahrscheinlichsten Differentialdiagnosen für diese Fragestellung auf.\\n\"\n",
    "        f\"Du hast 4 MRT-Protokolle zur Verfügung: {retrieved_docs}. Jedes Protokoll enthält einen Abschnitt zur Anwendung. Dieser behinhaltet für welche radiologischen Fragestellungen das jeweilige Protokoll geeignet ist.\\n\"\n",
    "        f\"Wähle das Protokoll aus, was am besten zur Beantwortung dieser radiologischen Fragestellung passt. Berücksichtige hierbei vorallem die Details aus Anwendungsbeschreibung des Protokolls.\\n\"\n",
    "        f\"Gib die Informationen des Protokolls unverändert wieder. Befolge stets das Antwortformat. Erkläre dein Vorgehen nicht, füge keine weiteren Zeichen hinzu.\\n\"\n",
    "        f\"Deine Antwort soll folgendes Format haben:\\n\"\n",
    "        f\"Abkürzungen: Abkürzung:ausgeschriebene Abkürzung, Abkürzung:ausgeschriebene Abkürzung, ...\\n\"\n",
    "        f\"Diagnose: bereits bekannte Diagnose, Differentialdiagnosen: 1. wahrscheinlichste, 2. zweitwahrscheinlichste, 3. drittwahrscheinlichste\\n\"\n",
    "        f\"Protokollname: Name des Protokolls\\n\"\n",
    "        f\"Sequenzen: Sequenzen des Protokolls\\n\"\n",
    "        f\"Kontrastmittelgabe: Kontrastmittelgabe im Protokoll\\n\"\n",
    "        f\"Radiologische Fragestellung: {row['Fragestellung']}\"\n",
    "        )\n",
    "    \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a neuroradiologist.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    result = completion.choices[0].message.content.strip()\n",
    "\n",
    "    return result, retrieved_docs\n",
    "\n",
    "# Iterate over the test DataFrame and generate predictions\n",
    "results = []\n",
    "retrieved_documents = []\n",
    "\n",
    "for index, row in df.iterrows():  \n",
    "    result, retrieved_docs = prediction_prompt(row)\n",
    "    results.append(result)\n",
    "    retrieved_documents.append(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT DATA OF THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs_name = []\n",
    "\n",
    "# Iterate through each list in the retrieved_documents and extract protocol name\n",
    "for entry_list in retrieved_documents:\n",
    "    name_group = []  \n",
    "\n",
    "    if isinstance(entry_list, list):\n",
    "        # Iterate through each document-like object in the list\n",
    "        for entry in entry_list:\n",
    "            # Check if the entry has 'page_content'\n",
    "            if hasattr(entry, 'page_content'):\n",
    "                page_content = entry.page_content\n",
    "                \n",
    "                # Split the content by lines\n",
    "                lines = page_content.split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    if re.match(r\"Name\\s*:\", line.strip()):\n",
    "                        # Extract the part after 'Name:'\n",
    "                        name = line.split(\":\", 1)[1].strip()\n",
    "                        name_group.append(name)\n",
    "\n",
    "    # Append the group of names (per row) to the main list\n",
    "    retrieved_docs_name.append(name_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the extracted data\n",
    "abbreviations = []\n",
    "diagnosis = []\n",
    "differential_diagnosis = []\n",
    "protocol_prediction = []\n",
    "sequence_prediction = []\n",
    "contrastmedium_prediction = []\n",
    "\n",
    "# Iterate through each entry in the results\n",
    "for entry in results:\n",
    "    # Match the 'Abkürzungen:' section\n",
    "    abbreviation_match = re.search(r'Abkürzungen:\\s*(.*?)\\n(?:Diagnose|Differentialdiagnosen):', entry, re.DOTALL)\n",
    "    if abbreviation_match:\n",
    "        abbreviations.append(abbreviation_match.group(1).strip())\n",
    "\n",
    "    # Match the 'Diagnose:' section\n",
    "    diagnosis_match = re.search(r'Diagnose:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if diagnosis_match:\n",
    "        diagnosis.append(diagnosis_match.group(1).strip())\n",
    "\n",
    "    # Match the 'Differentialdiagnosen:' section\n",
    "    dd_match = re.search(r'Diagnose:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if dd_match:\n",
    "        differential_diagnosis.append(dd_match.group(1).strip())\n",
    "    else:\n",
    "        differential_diagnosis.append(None)  # Append an None if the section is missing\n",
    "\n",
    "    # Match the 'Protokollname:' section\n",
    "    protocol_match = re.search(r'Protokollname:\\s*(.*?)\\n', entry, re.DOTALL)\n",
    "    if protocol_match:\n",
    "        protocol_prediction.append(protocol_match.group(1).strip())\n",
    "        \n",
    "    # Match the 'Sequenzen:' section\n",
    "    sequenzen_match = re.search(r'Sequenzen:\\s*(.*?)\\n(?:Kontrastmittelgabe):', entry, re.DOTALL)\n",
    "    if sequenzen_match:\n",
    "        sequence_prediction.append(sequenzen_match.group(1).strip())\n",
    "\n",
    "    # Match the 'Kontrastmittelgabe:' section\n",
    "    contrastmedium_match = re.search(r'Kontrastmittelgabe:\\s*(\\S+)', entry)\n",
    "    if contrastmedium_match:\n",
    "        contrastmedium_prediction.append(contrastmedium_match.group(1).strip())\n",
    "\n",
    "#Add extracted results to the dataframe\n",
    "df.insert(7,'Abkürzungen', abbreviations)\n",
    "df.insert(7,'Diagnosen', differential_diagnosis)\n",
    "df.insert(8,'Vorhersage Protokollname',protocol_prediction)\n",
    "df.insert(9,'Vorhersage Sequenzen', sequence_prediction)\n",
    "df.insert(10,'Vorhersage Kontrastmittelgabe', contrastmedium_prediction)\n",
    "df.insert(11,'Retrieved Documents',retrieved_docs_name)\n",
    "\n",
    "#Save as csv\n",
    "df.to_csv(path_or_buf='/path/results',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load results\n",
    "df_OS_woRAG = pd.read_csv(filepath_or_buffer='/path/results',sep=';')\n",
    "df_OS_RAG = pd.read_csv(filepath_or_buffer='/path/results',sep=';')\n",
    "df_woRAG = pd.read_csv(filepath_or_buffer='/path/results',sep=';')\n",
    "df_RAG = df_final = pd.read_csv(filepath_or_buffer='/path/results',sep=';')\n",
    "\n",
    "#Import Ground Truth\n",
    "df_GT = pd.read_csv(filepath_or_buffer='data/GT', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA 3.1 405B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function for calculation of token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Provide the data\n",
    "ground_truths = df_GT['Sequenzen'] \n",
    "results1 = df_OS_woRAG['Vorhersage Sequenzen']\n",
    "\n",
    "accuracies_OS_woRAG = []\n",
    "for gt, res in zip(ground_truths, results1):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_OS_woRAG.append(accuracy)\n",
    "\n",
    "#Calculate average accuracy\n",
    "average_accuracy_seq = sum(accuracies_OS_woRAG) / len(accuracies_OS_woRAG)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_OS_woRAG)\n",
    "\n",
    "#Contrast medium administration\n",
    "#Replace values and change dtype\n",
    "df_OS_woRAG['Vorhersage Kontrastmittelgabe'] = df_OS_woRAG['Vorhersage Kontrastmittelgabe'].replace({'ja': 1,'nein':0,'gegebenenfalls':0})\n",
    "df_OS_woRAG['Vorhersage Kontrastmittelgabe'] = df_OS_woRAG['Vorhersage Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_OS_woRAG = df_OS_woRAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_OS_woRAG.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_OS_woRAG) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_OS_woRAG)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_OS_woRAG, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "data_results = [{\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':'-',\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium':'-',\n",
    "    'Number of Correct Retrieval': '-',\n",
    "    'Number of Correct Protocol': '-',\n",
    "    'Protocol/Retrieval': '-'\n",
    "}]\n",
    "index = ['LLama 3.1 wo RAG']\n",
    "\n",
    "df_results = pd.DataFrame(data_results, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA 3.1 405B WITH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of Retrieval\n",
    "# Function to normalize the protocol names \n",
    "def normalize_name(name):\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Normalize spaces around dashes and parentheses, and remove extra spaces\n",
    "    name = re.sub(r'\\s*-\\s*', '-', name)  # normalize spaces around dashes\n",
    "    name = re.sub(r'\\s*\\(\\s*', '(', name)  # normalize spaces before '('\n",
    "    name = re.sub(r'\\s*\\)\\s*', ')', name)  # normalize spaces after ')'\n",
    "    name = re.sub(r\"'\", '', name)  # remove apostrophes\n",
    "\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()  # Remove extra spaces\n",
    "    return name\n",
    "\n",
    "# Function to check if the normalized protocol name in ground truth is exactly contained in the normalized names of the corresponding row in results\n",
    "def check_name_in_list2(list_one, list_two):\n",
    "    results = []\n",
    "    \n",
    "    for i, name_one in enumerate(list_one):\n",
    "        found = False\n",
    "        # Normalize the name in ground truth\n",
    "        normalized_name_one = normalize_name(name_one)\n",
    "        \n",
    "        # Get the corresponding row in results\n",
    "        if i < len(list_two):\n",
    "            row_two = [list_two[i]]\n",
    "\n",
    "            \n",
    "            # Iterate over the names in the corresponding row in results\n",
    "            for name_two in row_two:\n",
    "                # Normalize the name in results\n",
    "                normalized_name_two = normalize_name(name_two)\n",
    "        \n",
    "                # Check if names match\n",
    "                if normalized_name_one == normalized_name_two:\n",
    "                    found = True\n",
    "                    break\n",
    "        \n",
    "       \n",
    "        results.append(found)\n",
    "    \n",
    "    return results\n",
    "\n",
    "boolean_results = check_name_in_list2(df_GT['Protokolleinstufung'], df_OS_RAG['Retrieved Documents'])\n",
    "\n",
    "correct_protocol = sum(boolean_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI Sequences \n",
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function for calculation of token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Provide the data\n",
    "ground_truths = df_GT['Sequenzen'] \n",
    "results2 = df_OS_RAG['Vorhersage Sequenzen']\n",
    "\n",
    "accuracies_OS_RAG = []\n",
    "for gt, res in zip(ground_truths, results2):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_OS_RAG.append(accuracy)\n",
    "\n",
    "#Calculate accuracy\n",
    "average_accuracy_seq = sum(accuracies_OS_RAG) / len(accuracies_OS_RAG)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_OS_RAG)\n",
    "\n",
    "#Contrast Medium Administration\n",
    "#Replace values and change dtype\n",
    "df_OS_RAG['Vorhersage Kontrastmittelgabe'] = df_OS_RAG['Vorhersage Kontrastmittelgabe'].replace({'ja': 1,'nein':0,'gegebenenfalls':0})\n",
    "df_OS_RAG['Vorhersage Kontrastmittelgabe'] = df_OS_RAG['Vorhersage Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_OS_RAG = df_OS_RAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_OS_RAG.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_OS_RAG) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_OS_RAG)\n",
    "\n",
    "# Perform bootstrapping to calculate confidence interval\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_OS_RAG, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# Printing results\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['LLAMA 3.1 with RAG'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':'-',\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium':'-',\n",
    "    'Number of Correct Retrieval': correctly_retrieved,\n",
    "    'Number of Correct Protocol': correct_protocol,\n",
    "    'Protocol/Retrieval':accuracy_correctprotocol_out_of_right_retrieval\n",
    "}\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI sequences \n",
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function to calculate token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Provide the data\n",
    "ground_truths = df_GT['Sequenzen'] \n",
    "results3 = df_woRAG['Vorhersage Sequenzen']\n",
    "\n",
    "accuracies_woRAG = []\n",
    "for gt, res in zip(ground_truths, results3):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_woRAG.append(accuracy)\n",
    "\n",
    "#Calculate accuracy\n",
    "average_accuracy_seq = sum(accuracies_woRAG) / len(accuracies_woRAG)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_woRAG)\n",
    "\n",
    "#Contrast Medium Administration\n",
    "#Replace values and change dtype\n",
    "df_woRAG['Vorhersage Kontrastmittelgabe'] = df_woRAG['Vorhersage Kontrastmittelgabe'].replace({'ja': 1,'nein':0,'gegebenenfalls':0})\n",
    "df_woRAG['Vorhersage Kontrastmittelgabe'] = df_woRAG['Vorhersage Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_woRAG = df_woRAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_woRAG.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_woRAG) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_woRAG)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_woRAG, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['GPT-4o wo RAG'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':'-',\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium':'-',\n",
    "    'Number of Correct Retrieval': '-',\n",
    "    'Number of Correct Protocol': '-',\n",
    "    'Protocol/Retrieval': '-'\n",
    "}\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o WITH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Retrieval\n",
    "# Function to normalize the protocol names \n",
    "def normalize_name(name):\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Normalize spaces around dashes and parentheses, and remove extra spaces\n",
    "    name = re.sub(r'\\s*-\\s*', '-', name)  # normalize spaces around dashes\n",
    "    name = re.sub(r'\\s*\\(\\s*', '(', name)  # normalize spaces before '('\n",
    "    name = re.sub(r'\\s*\\)\\s*', ')', name)  # normalize spaces after ')'\n",
    "    name = re.sub(r\"'\", '', name)  # remove apostrophes\n",
    "\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()  # Remove extra spaces\n",
    "    return name\n",
    "\n",
    "# Function to check if the normalized protocol name in ground truth is exactly contained in the normalized names of the corresponding row in results\n",
    "def check_name_in_list2(list_one, list_two):\n",
    "    results = []\n",
    "    \n",
    "    for i, name_one in enumerate(list_one):\n",
    "        found = False\n",
    "        # Normalize the name in ground truth\n",
    "        normalized_name_one = normalize_name(name_one)\n",
    "        \n",
    "        # Get the corresponding row in results\n",
    "        if i < len(list_two):\n",
    "            row_two = [list_two[i]]\n",
    "\n",
    "            \n",
    "            # Iterate over the names in the corresponding row in results\n",
    "            for name_two in row_two:\n",
    "                # Normalize the name in results\n",
    "                normalized_name_two = normalize_name(name_two)\n",
    "        \n",
    "                # Check if names match\n",
    "                if normalized_name_one == normalized_name_two:\n",
    "                    found = True\n",
    "                    break\n",
    "        \n",
    "       \n",
    "        results.append(found)\n",
    "    \n",
    "    return results\n",
    "\n",
    "boolean_results = check_name_in_list2(df_GT['Protokolleinstufung'], df_RAG['Retrieved Documents'])\n",
    "\n",
    "correct_protocol = sum(boolean_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI Sequences \n",
    "#Tokenize data\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function to calculate token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Provide the data\n",
    "ground_truths = df_GT['Sequenzen'] \n",
    "results4 = df_RAG['Vorhersage Sequenzen']\n",
    "\n",
    "accuracies_RAG = []\n",
    "for gt, res in zip(ground_truths, results4):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_RAG.append(accuracy)\n",
    "\n",
    "#Calculate accuracy\n",
    "average_accuracy_seq = sum(accuracies_RAG) / len(accuracies_RAG)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_RAG)\n",
    "\n",
    "#Contrast Medium Administration\n",
    "#Replace values and change dtype\n",
    "df_RAG['Vorhersage Kontrastmittelgabe'] = df_RAG['Vorhersage Kontrastmittelgabe'].replace({'ja': 1,'nein':0,'gegebenenfalls':0})\n",
    "df_RAG['Vorhersage Kontrastmittelgabe'] = df_RAG['Vorhersage Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_RAG = df_RAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_RAG.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_RAG) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_RAG)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_RAG, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['GPT with RAG'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':'-',\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium':'-',\n",
    "    'Number of Correct Retrieval': correctly_retrieved,\n",
    "    'Number of Correct Protocol': correct_protocol,\n",
    "    'Protocol/Retrieval':accuracy_correctprotocol_out_of_right_retrieval\n",
    "}\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RADIOLOGISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(filepath_or_buffer='/path/GT',sep=';')\n",
    "df2= pd.read_csv(filepath_or_buffer='/path/selection resident 2', sep=';')\n",
    "df3 = pd.read_csv(filepath_or_buffer='/path/selection resident 1', sep=';')\n",
    "df4 = pd.read_csv(filepath_or_buffer='/path/selection radiologist 2', sep=';')\n",
    "df5  = pd.read_csv(filepath_or_buffer='path/selection radiologist 1', sep=';')\n",
    "df_GT = df1.fillna('')\n",
    "df_resident2 = df2.fillna('')\n",
    "df_resident1 = df3.fillna('')\n",
    "df_radiologist2 = df4.fillna('')\n",
    "df_radiologist1 = df5.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiologist 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI Sequences\n",
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function to calculate token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Example usage with your provided data:\n",
    "ground_truths = df_GT['Sequenzen']\n",
    "results = df_radiologist1['Sequenzen'] \n",
    "\n",
    "accuracies_rad1 = []\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_rad1.append(accuracy)\n",
    "\n",
    "average_accuracy_seq = sum(accuracies_rad1) / len(accuracies_rad1)\n",
    "\n",
    "# Cohen's Kappa calculation\n",
    "# Create agreement labels: 1 if they agree (accuracy == 1), else 0\n",
    "agreement_labels_ground_truth = []\n",
    "agreement_labels_result = []\n",
    "\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    # Tokenize and sort to compare token-based categories\n",
    "    tokenized_gt = tokenize(gt)\n",
    "    tokenized_res = tokenize(res)\n",
    "    \n",
    "    # Label agreement: 1 if identical tokens, else 0\n",
    "    agreement_labels_ground_truth.append(' '.join(sorted(tokenized_gt)))\n",
    "    agreement_labels_result.append(' '.join(sorted(tokenized_res)))\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_seq = cohen_kappa_score(agreement_labels_ground_truth, agreement_labels_result)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_rad1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrast Medium Administration\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_radiologist1['Kontrastmittelgabe'] = df_radiologist1['Kontrastmittelgabe'].astype(bool)\n",
    "df_GT['Kontrastmittelgabe'] = df_GT['Kontrastmittelgabe'].astype(bool)\n",
    "df_equal_rad1 = df_radiologist1['Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe']\n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_rad1.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_rad1) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_rad1)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_rad1, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_cm = cohen_kappa_score(df_radiologist1['Kontrastmittelgabe'], df_GT['Kontrastmittelgabe'])\n",
    "\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['Radiologist 1'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':kappa_score_seq,\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium': kappa_score_cm,\n",
    "    'Number of Correct Retrieval': '-',\n",
    "    'Number of Correct Protocol': '-',\n",
    "    'Protocol/Retrieval':'-'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiologist 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI Sequences \n",
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function to calculate token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Example usage with your provided data:\n",
    "ground_truths = df_GT['Sequenzen']\n",
    "results = df_radiologist2['Sequenzen'] \n",
    "\n",
    "accuracies_rad2 = []\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_rad2.append(accuracy)\n",
    "\n",
    "average_accuracy_seq = sum(accuracies_rad2) / len(accuracies_rad2)\n",
    "\n",
    "# Cohen's Kappa calculation\n",
    "# Create agreement labels: 1 if they agree (accuracy == 1), else 0\n",
    "agreement_labels_ground_truth = []\n",
    "agreement_labels_result = []\n",
    "\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    # Tokenize and sort to compare token-based categories\n",
    "    tokenized_gt = tokenize(gt)\n",
    "    tokenized_res = tokenize(res)\n",
    "    \n",
    "    # Label agreement: 1 if identical tokens, else 0\n",
    "    agreement_labels_ground_truth.append(' '.join(sorted(tokenized_gt)))\n",
    "    agreement_labels_result.append(' '.join(sorted(tokenized_res)))\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_seq = cohen_kappa_score(agreement_labels_ground_truth, agreement_labels_result)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_rad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrast Medium Administration\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_rad2 = df_radiologist2['Kontrastmittelgabe'].astype(bool) == df_GT['Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_rad2.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_rad2) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_rad2)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_rad2, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_cm = cohen_kappa_score(df_radiologist2['Kontrastmittelgabe'].astype(bool), df_GT['Kontrastmittelgabe'].astype(bool))\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['Radiologist 2'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':kappa_score_seq,\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium': kappa_score_cm,\n",
    "    'Number of Correct Retrieval': '-',\n",
    "    'Number of Correct Protocol': '-',\n",
    "    'Protocol/Retrieval':'-'\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resident 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI Sequences\n",
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function to calculate token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Example usage with your provided data:\n",
    "ground_truths = df_GT['Sequenzen']\n",
    "results = df_resident1['Sequenzen'] \n",
    "\n",
    "accuracies_res1 = []\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_res1.append(accuracy)\n",
    "\n",
    "average_accuracy_seq = sum(accuracies_res1) / len(accuracies_res1)\n",
    "\n",
    "# Cohen's Kappa calculation\n",
    "# Create agreement labels: 1 if they agree (accuracy == 1), else 0\n",
    "agreement_labels_ground_truth = []\n",
    "agreement_labels_result = []\n",
    "\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    # Tokenize and sort to compare token-based categories\n",
    "    tokenized_gt = tokenize(gt)\n",
    "    tokenized_res = tokenize(res)\n",
    "    \n",
    "    # Label agreement: 1 if identical tokens, else 0\n",
    "    agreement_labels_ground_truth.append(' '.join(sorted(tokenized_gt)))\n",
    "    agreement_labels_result.append(' '.join(sorted(tokenized_res)))\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_seq = cohen_kappa_score(agreement_labels_ground_truth, agreement_labels_result)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrast Medium Administration\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_res1 = df_resident1['Kontrastmittelgabe'].astype(bool) == df_GT['Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_res1.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_res1) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_res1)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_res1, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_cm = cohen_kappa_score(df_resident1['Kontrastmittelgabe'].astype(bool), df_GT['Kontrastmittelgabe'].astype(bool))\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['Resident 1'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':kappa_score_seq,\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium': kappa_score_cm,\n",
    "    'Number of Correct Retrieval': '-',\n",
    "    'Number of Correct Protocol': '-',\n",
    "    'Protocol/Retrieval':'-'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resident 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRI Sequences\n",
    "#Tokenize sequences\n",
    "def tokenize(text):\n",
    "    tokens = [token.strip() for token in text.split(',')]\n",
    "    return tokens\n",
    "\n",
    "#Function to calculate token-based accuracy\n",
    "def calculate_symmetric_token_based_accuracy(ground_truth, result):\n",
    "    ground_truth_tokens = tokenize(ground_truth)\n",
    "    result_tokens = tokenize(result)\n",
    "    \n",
    "    ground_truth_counts = Counter(ground_truth_tokens)\n",
    "    result_counts = Counter(result_tokens)\n",
    "    \n",
    "    # Calculate the number of matching tokens considering repetitions\n",
    "    matching_tokens_gt_to_res = sum(min(ground_truth_counts[token], result_counts[token]) for token in ground_truth_counts)\n",
    "    matching_tokens_res_to_gt = sum(min(ground_truth_counts[token], result_counts[token]) for token in result_counts)\n",
    "    \n",
    "    # Total tokens in ground truth and result (considering repetitions)\n",
    "    total_tokens_gt = sum(ground_truth_counts.values())\n",
    "    total_tokens_res = sum(result_counts.values())\n",
    "    \n",
    "    # Calculate accuracy from ground truth to result\n",
    "    accuracy_gt_to_res = matching_tokens_gt_to_res / total_tokens_gt if total_tokens_gt > 0 else 0\n",
    "    \n",
    "    # Calculate accuracy from result to ground truth\n",
    "    accuracy_res_to_gt = matching_tokens_res_to_gt / total_tokens_res if total_tokens_res > 0 else 0\n",
    "    \n",
    "    # Symmetric accuracy: average of both directions\n",
    "    symmetric_accuracy = (accuracy_gt_to_res + accuracy_res_to_gt) / 2\n",
    "    \n",
    "    return symmetric_accuracy\n",
    "\n",
    "# Example usage with your provided data:\n",
    "ground_truths = df_GT['Sequenzen']\n",
    "results = df_resident2['Sequenzen'] \n",
    "\n",
    "accuracies_res2 = []\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    accuracy = calculate_symmetric_token_based_accuracy(gt, res)\n",
    "    accuracies_res2.append(accuracy)\n",
    "\n",
    "average_accuracy_seq = sum(accuracies_res2) / len(accuracies_res2)\n",
    "\n",
    "# Cohen's Kappa calculation\n",
    "# Create agreement labels: 1 if they agree (accuracy == 1), else 0\n",
    "agreement_labels_ground_truth = []\n",
    "agreement_labels_result = []\n",
    "\n",
    "for gt, res in zip(ground_truths, results):\n",
    "    # Tokenize and sort to compare token-based categories\n",
    "    tokenized_gt = tokenize(gt)\n",
    "    tokenized_res = tokenize(res)\n",
    "    \n",
    "    # Label agreement: 1 if identical tokens, else 0\n",
    "    agreement_labels_ground_truth.append(' '.join(sorted(tokenized_gt)))\n",
    "    agreement_labels_result.append(' '.join(sorted(tokenized_res)))\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_seq = cohen_kappa_score(agreement_labels_ground_truth, agreement_labels_result)\n",
    "\n",
    "# Bootstrap function to calculate confidence intervals\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Resample with replacement and calculate means\n",
    "    sample_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
    "\n",
    "    # Calculate the percentiles for the given confidence level\n",
    "    lower_bound = np.percentile(sample_means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(sample_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "confidence_interval_seq = bootstrap_confidence_interval(accuracies_res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrast Medium Administration\n",
    "# Comparing the two columns and calculating accuracy\n",
    "df_equal_res2 = df_resident2['Kontrastmittelgabe'].astype(bool) == df_GT['Kontrastmittelgabe'].astype(bool)\n",
    "\n",
    "# Calculating the number of correct predictions (where the comparison is True)\n",
    "correct_predictions = df_equal_res2.sum()\n",
    "\n",
    "# Calculating accuracy\n",
    "average_accuracy_cm = correct_predictions / len(df_equal_res2) * 100\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "# Number of total samples\n",
    "n = len(df_equal_res2)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(n_iterations):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = np.random.choice(df_equal_res2, size=n, replace=True)\n",
    "    \n",
    "    # Calculate accuracy for this bootstrap sample\n",
    "    accuracies = np.sum(bootstrap_sample) / len(bootstrap_sample) * 100\n",
    "    bootstrap_accuracies.append(accuracies)\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrap_accuracies = np.array(bootstrap_accuracies)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval_cm = np.percentile(bootstrap_accuracies, [2.5, 97.5])\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa_score_cm = cohen_kappa_score(df_resident2['Kontrastmittelgabe'].astype(bool), df_GT['Kontrastmittelgabe'].astype(bool))\n",
    "\n",
    "# New results to add\n",
    "df_results.loc['Resident 2'] = {\n",
    "    'Confidenz Interval Sequences': (confidence_interval_seq[0] * 100, confidence_interval_seq[1] * 100),\n",
    "    'Average Accuracy Sequences': average_accuracy_seq * 100,\n",
    "    'Kappa Score Sequences':kappa_score_seq,\n",
    "    'Confidenz Interval Contrastmedium': (confidence_interval_cm[0], confidence_interval_cm[1]),\n",
    "    'Average Accuracy Contrastmedium': average_accuracy_cm,\n",
    "    'Kappa Score Contrastmedium': kappa_score_cm,\n",
    "    'Number of Correct Retrieval': '-',\n",
    "    'Number of Correct Protocol': '-',\n",
    "    'Protocol/Retrieval':'-'\n",
    "}\n",
    "\n",
    "df_results.to_csv(path_or_buf='/Users/laranoellereiner/Projekte/Promotionsdaten/Promotion/Results/Final Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARISON BETWEEN MODEL RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiologists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracies = []\n",
    "\n",
    "# Loop through the values in all accuracy lists simultaneously\n",
    "for res2, res1, rad1, rad2 in zip(accuracies_res2, accuracies_res1, accuracies_rad1, accuracies_rad2):\n",
    "    accuracy = (res2 + res1 + rad1 + rad2) / 4 \n",
    "    mean_accuracies.append(accuracy) \n",
    "\n",
    "mean_accuracies_CM = []\n",
    "\n",
    "# Loop through the values in all accuracy lists simultaneously\n",
    "for res2, res1, rad1, rad2 in zip(df_equal_rad1, df_equal_rad2, df_equal_res1, df_equal_res2):\n",
    "    accuracy = (res2 + res1 + rad1 + rad2) / 4  # Calculate the average accuracy\n",
    "    mean_accuracies_CM.append(accuracy)  # Append the result to mean_accuracy list\n",
    "\n",
    "# Calculate standard deviation for both sets of accuracies\n",
    "sd_mean_accuracy = np.std(mean_accuracies)\n",
    "sd_mean_accuracy_CM = np.std(mean_accuracies_CM)\n",
    "\n",
    "#Calculate mean accuracy \n",
    "mean_accuracy = sum(mean_accuracies)/len(mean_accuracies)\n",
    "mean_accuracy_CM = sum(mean_accuracies_CM)/len(mean_accuracies_CM)\n",
    "\n",
    "print(mean_accuracy)\n",
    "print(mean_accuracy_CM)\n",
    "print(sd_mean_accuracy)\n",
    "print(sd_mean_accuracy_CM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA 3.1 Sequences non-RAG to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_accuracies = accuracies_OS_woRAG\n",
    "model_b_accuracies = accuracies_OS_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "comparison_results = [{\n",
    "    'P-Value': p_value\n",
    "}]\n",
    "index = ['LLama 3.1 Sequences non-RAG to RAG']\n",
    "\n",
    "df_comparison_results = pd.DataFrame(comparison_results, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA 3.1 Contrastmedium non-RAG to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the two columns \n",
    "df_equal_OS_woRAG = df_OS_woRAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "df_equal_OS_RAG = df_OS_RAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Lists of boolean predictions from two LLMs\n",
    "llm1_predictions = df_equal_OS_RAG\n",
    "llm2_predictions = df_equal_OS_woRAG\n",
    "\n",
    "# Initialize counts for the 2x2 contingency table\n",
    "a = b = c = d = 0\n",
    "\n",
    "# Compare each prediction pair and update contingency table counts\n",
    "for pred1, pred2 in zip(llm1_predictions, llm2_predictions):\n",
    "    if pred1 == True and pred2 == True:\n",
    "        a += 1\n",
    "    elif pred1 == True and pred2 == False:\n",
    "        b += 1\n",
    "    elif pred1 == False and pred2 == True:\n",
    "        c += 1\n",
    "    elif pred1 == False and pred2 == False:\n",
    "        d += 1\n",
    "\n",
    "# Create the contingency table\n",
    "table = np.array([[a, b],\n",
    "                  [c, d]])\n",
    "\n",
    "# Perform the McNemar test\n",
    "result = mcnemar(table, exact=False)  # Set exact=True for small samples\n",
    "\n",
    "# Output the test statistic and p-value\n",
    "print(f'Contingency table: \\n{table}')\n",
    "print(f'Chi-squared: {result.statistic}, p-value: {result.pvalue}')\n",
    "p_value = (result.pvalue)\n",
    "\n",
    "df_comparison_results.loc['LLama 3.1 Contrastmedium non-RAG to RAG']= {'P-Value':p_value}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Sequences non-RAG to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_accuracies = accuracies_woRAG\n",
    "model_b_accuracies = accuracies_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "df_comparison_results.loc['GPT Sequences non-RAG to RAG'] = {'P-Value':p_value}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Contrastmedium non-RAG to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the two columns \n",
    "df_equal_woRAG = df_woRAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "df_equal_RAG = df_RAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Lists of boolean predictions from two LLMs\n",
    "llm1_predictions = df_equal_woRAG\n",
    "llm2_predictions = df_equal_RAG\n",
    "\n",
    "# Initialize counts for the 2x2 contingency table\n",
    "a = b = c = d = 0\n",
    "\n",
    "# Compare each prediction pair and update contingency table counts\n",
    "for pred1, pred2 in zip(llm1_predictions, llm2_predictions):\n",
    "    if pred1 == True and pred2 == True:\n",
    "        a += 1\n",
    "    elif pred1 == True and pred2 == False:\n",
    "        b += 1\n",
    "    elif pred1 == False and pred2 == True:\n",
    "        c += 1\n",
    "    elif pred1 == False and pred2 == False:\n",
    "        d += 1\n",
    "\n",
    "# Create the contingency table\n",
    "table = np.array([[a, b],\n",
    "                  [c, d]])\n",
    "\n",
    "# Perform the McNemar test\n",
    "result = mcnemar(table, exact=False)  # Set exact=True for small samples\n",
    "\n",
    "# Output the test statistic and p-value\n",
    "print(f'Contingency table: \\n{table}')\n",
    "print(f'Chi-squared: {result.statistic}, p-value: {result.pvalue}')\n",
    "p_value = result.pvalue\n",
    "\n",
    "df_comparison_results.loc['GPT Contrastmedium non-RAG to RAG']= {'P-Value':p_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA 3.1 RAG to GPT RAG Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_accuracies = accuracies_OS_RAG\n",
    "model_b_accuracies = accuracies_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "df_comparison_results.loc['LLama RAG to GPT RAG Sequences']= {'P-Value':p_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA 3.1 RAG to GPT RAG Contrastmedium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the two columns \n",
    "df_equal_OS_RAG = df_OS_RAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "df_equal_RAG = df_RAG['Vorhersage Kontrastmittelgabe'] == df_GT['Kontrastmittelgabe'] \n",
    "\n",
    "# Lists of boolean predictions from two LLMs\n",
    "llm1_predictions = df_equal_OS_RAG\n",
    "llm2_predictions = df_equal_RAG\n",
    "\n",
    "# Initialize counts for the 2x2 contingency table\n",
    "a = b = c = d = 0\n",
    "\n",
    "# Compare each prediction pair and update contingency table counts\n",
    "for pred1, pred2 in zip(llm1_predictions, llm2_predictions):\n",
    "    if pred1 == True and pred2 == True:\n",
    "        a += 1\n",
    "    elif pred1 == True and pred2 == False:\n",
    "        b += 1\n",
    "    elif pred1 == False and pred2 == True:\n",
    "        c += 1\n",
    "    elif pred1 == False and pred2 == False:\n",
    "        d += 1\n",
    "\n",
    "# Create the contingency table\n",
    "table = np.array([[a, b],\n",
    "                  [c, d]])\n",
    "\n",
    "# Perform the McNemar test\n",
    "result = mcnemar(table, exact=False)  # Set exact=True for small samples\n",
    "\n",
    "# Output the test statistic and p-value\n",
    "print(f'Contingency table: \\n{table}')\n",
    "print(f'Chi-squared: {result.statistic}, p-value: {result.pvalue}')\n",
    "p_value = result.pvalue\n",
    "\n",
    "\n",
    "df_comparison_results.loc['LLama RAG to GPT RAG Contrastmedium']= {'P-Value':p_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiologists to GPT-4o with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_accuracies = mean_accuracies\n",
    "model_b_accuracies = accuracies_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "df_comparison_results.loc['Radiologists to GPT RAG Sequences']= {'P-Value':p_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_CM_RAG = df_RAG['Vorhersage Kontrastmittelgabe'].astype(float)\n",
    "\n",
    "\n",
    "# Example lists of accuracies for two models\n",
    "model_a_accuracies = mean_accuracies_CM\n",
    "model_b_accuracies = accuracy_CM_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "p_value = result.pvalue\n",
    "\n",
    "df_comparison_results.loc['Radiologists to GPT RAG Contrastmedium']= {'P-Value':p_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiologists to LLama 3.1 with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_accuracies = mean_accuracies\n",
    "model_b_accuracies = accuracies_OS_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "df_comparison_results.loc['Radiologists to LLama RAG Sequences']= {'P-Value':p_value}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_CM_OS_RAG = df_OS_RAG['Vorhersage Kontrastmittelgabe'].astype(float)\n",
    "\n",
    "\n",
    "# Example lists of accuracies for two models\n",
    "model_a_accuracies = mean_accuracies_CM\n",
    "model_b_accuracies = accuracy_CM_OS_RAG\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(model_a_accuracies, model_b_accuracies)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Wilcoxon test statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "p_value = result.pvalue\n",
    "\n",
    "df_comparison_results.loc['Radiologists to LLama RAG Contrastmedium']= {'P-Value':p_value}\n",
    "\n",
    "df_comparison_results.to_csv(path_or_buf='/Users/laranoellereiner/Projekte/Promotionsdaten/Promotion/Results/Comparison Results.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
